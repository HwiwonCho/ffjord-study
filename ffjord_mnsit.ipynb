{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a54c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torchdiffeq import  odeint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Batch_Size = 32\n",
    "data_dir = './data'\n",
    "os.makedirs(data_dir, exist_ok=True) # ë°ì´í„° í´ë” ìƒì„±\n",
    "\n",
    "INPUT_DIM = 28 * 28 \n",
    "TIME_T = 1.0\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. FFJORD ìµœì í™” ì „ì²˜ë¦¬ ì •ì˜ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)\n",
    "# ----------------------------------------------------------------------\n",
    "# MNIST ë°ì´í„°ì˜ í‰ê· (Mean)ê³¼ í‘œì¤€í¸ì°¨(Std)ëŠ” 0.5ë¥¼ ì‚¬ìš©í•˜ì—¬ [-1, 1] ë²”ìœ„ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.\n",
    "class AddNoise(object):\n",
    "    def __call__(self, tensor):\n",
    "        # 0 ~ 1/256 ì‚¬ì´ì˜ ê· ë“± ë¶„í¬ ë…¸ì´ì¦ˆ ì¶”ê°€ ì¸ë±ìŠ¤ëŠ” ì—°ì†ì ì´ì—¬ì•¼ í•œë‹¤\n",
    "        return tensor + torch.rand_like(tensor) / 256.0\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),       # [0, 1] ë²”ìœ„ë¡œ ë³€í™˜\n",
    "    AddNoise(),                  # ğŸŸ¢ [í•„ìˆ˜] ë…¸ì´ì¦ˆ ì¶”ê°€ (ê¼¼ìˆ˜ ë°©ì§€)\n",
    "    transforms.Normalize((0.5,), (0.5,)) # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n",
    "])\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. ë°ì´í„° ë¡œë” ì •ì˜ (ìˆ˜ì •ëœ transform ì ìš©)\n",
    "# ----------------------------------------------------------------------\n",
    "train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Batch_Size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bca06223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì…ë ¥ìœ¼ë¡œ f(x,t)ì™€ y, të¥¼ ë°›ì•„ í—ˆí‚¨ìŠ¨ ì¶”ì •ëŸ‰ì„ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜\n",
    "#ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•˜ì—¬ í‰ê· ì„ ë‚´ì–´ ì •í™•ë„ë¥¼ ë†’ì„\n",
    "def Huchinson_estimator(forwardzt, y,  num_epsilons):\n",
    "    Batch_Size=y.shape[0]\n",
    "    grad_avg=torch.zeros(y.shape[0],1).to(device).to(y.dtype)\n",
    "    for i in range(num_epsilons):\n",
    "        epsilon=torch.randn_like(y).to(device).to(y.dtype)\n",
    "        grad = torch.autograd.grad(forwardzt, y, grad_outputs=epsilon, create_graph=True)[0]\n",
    "        trace_estimate = torch.sum(grad*epsilon, dim=1, keepdim=True)\n",
    "        grad_avg += trace_estimate\n",
    "    grad_avg /= num_epsilons\n",
    "    return grad_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tê¹Œì§€ ode solve í•´ì£¼ê³  z(t)ì™€ trace ë°˜í™˜. Huchinson_estimator ì‚¬ìš©\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(dim+1, 128),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, dim),\n",
    "        ) \n",
    "        self.dims = dim\n",
    "\n",
    "    def forward1(self, t, x):\n",
    "        # t: scalar tensor on GPU\n",
    "        # x: (B, dim)\n",
    "        t_tensor = t.expand(x.shape[0], 1)  # repeat for batch\n",
    "        xt = torch.cat([x, t_tensor], dim=1)\n",
    "        return self.net(xt)\n",
    "    def forward(self, t, x):\n",
    "        z, logpz_T = x\n",
    "\n",
    "        # force t to GPU\n",
    "        if not torch.is_tensor(t):\n",
    "            t = torch.tensor([t], device=z.device, dtype=z.dtype)\n",
    "        else:\n",
    "            t = t.to(z.device).to(z.dtype)\n",
    "\n",
    "        z_flat = z.view(z.size(0), -1)\n",
    "\n",
    "        forwardzt = self.forward1(t, z_flat)\n",
    "        trace = Huchinson_estimator(forwardzt, z_flat, num_epsilons=2)\n",
    "\n",
    "        return forwardzt, -trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì „ì²´ ì‹ ê²½ë§\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, ode_func, T=1.0):\n",
    "        super().__init__()\n",
    "        self.ode_func = ode_func\n",
    "        self.time_t = T\n",
    "        self.rtol=1e-5\n",
    "        self.atol=1e-5\n",
    "    #ìˆœë°©í–¥ ì ë¶„ ìƒì„±ê³¼ì •\n",
    "    def forward(self, z0, logpz=None):\n",
    "        if logpz is None:\n",
    "            logpz = torch.zeros(z0.shape[0],1).to(z0.device)\n",
    "        time_tensor=torch.tensor([0.0, self.time_t], dtype=torch.float).to(z0.device)\n",
    "        state_t = odeint(\n",
    "            self.ode_func,\n",
    "            (z0, logpz),\n",
    "            time_tensor,\n",
    "            method='Dopri5',\n",
    "            rtol=self.rtol,\n",
    "            atol=self.atol,\n",
    "            options={}\n",
    "        )\n",
    "        zT=state_t[0][-1]\n",
    "        logpz_T=state_t[1][-1]\n",
    "\n",
    "        return zT, logpz_T\n",
    "    #ì—­ë°©í–¥ ì ë¶„ í•™ìŠµê³¼ì •\n",
    "    def inverse(self, zT, logpz_T=None):\n",
    "        zT = zT.requires_grad_(True)\n",
    "        if logpz_T is None:\n",
    "            logpz_T = torch.zeros(zT.shape[0],1).to(zT.device)\n",
    "        time_tensor=torch.tensor([self.time_t, 0.0],dtype=torch.float).to(zT.device)\n",
    "        state_t = odeint( \n",
    "            #ìœ„ì— êµ¬í˜„í•œ ODEFunc ì‚¬ìš©           \n",
    "            self.ode_func,\n",
    "            # ODEì˜ ì´ˆê¸° ìƒíƒœ (zT, logpz_T(traceì— í•´ë‹¹))\n",
    "            (zT, logpz_T),\n",
    "            #ì‹œê°„ ì—­ë°©í–¥ìœ¼ë¡œ ì ë¶„\n",
    "            time_tensor,\n",
    "            method='Dopri5',\n",
    "            rtol=self.rtol,\n",
    "            atol=self.atol,\n",
    "            options={}\n",
    "        )\n",
    "        #ê°ê° forwardztì™€ traceê°€ ì ë¶„ëœ ê²°ê³¼ ë°˜í™˜\n",
    "        z0 = state_t[0][-1]      # ìµœì¢… ìƒíƒœ z0 (t=0, ë…¸ì´ì¦ˆ)\n",
    "        logpz_0 = state_t[1][-1] # ìµœì¢… ë¡œê·¸ í™•ë¥  ë³´ì¡° í•­ (log p(x) ê³„ì‚°ì— ì‚¬ìš©)\n",
    "        \n",
    "        return z0, logpz_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f8a8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í‘œì¤€ ì •ê·œë¶„í¬ì˜ ë¡œê·¸ í™•ë¥  ë°€ë„ í•¨ìˆ˜ ê³„ì‚°\n",
    "def get_logpz(z):\n",
    "    logpz = -0.5 * torch.sum(z**2, dim=1, keepdim=True) - 0.5 * z.shape[1] * torch.tensor(np.log(2 * np.pi)).to(device)\n",
    "    return logpz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2adbecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (epoch, model, optimizer, train_loader, device, INPUT_DIM):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        data = data.view(data.shape[0], INPUT_DIM)\n",
    "\n",
    "\n",
    "        #ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "        #ëª¨ë¸ì˜ ì—­ë°©í–¥ ì ë¶„ ìˆ˜í–‰(ë°ì´íƒ€ë¶„í¬->í‘œì¤€ì •ê·œë¶„í¬) ì´ z0ì´ ë‚˜ì˜¬í™•ë¥ ì„ ìµœëŒ€í™” í•´ì•¼í•œë‹¤.\n",
    "        data = data.view(data.shape[0], INPUT_DIM)\n",
    "        z0, logpz_0 = model.inverse(data)\n",
    "        #z0ì— ëŒ€í•œ í‘œì¤€ ì •ê·œë¶„í¬ì˜ ë¡œê·¸ í™•ë¥  ë°€ë„ í•¨ìˆ˜ ê³„ì‚°\n",
    "        logpz = get_logpz(z0.view(z0.size(0), -1))\n",
    "        #log p(x)= logp(z0) + log|det(dz0/dx)|(ì¦‰ -traceì˜ ì ë¶„ê°’)\n",
    "        logpx = logpz + logpz_0\n",
    "        #ë°°ì¹˜ì•ˆ ì „ì²´ì˜ lossí•¨ìˆ˜ í‰ê·  ê³„ì‚°\n",
    "        loss = -torch.mean(logpx)\n",
    "        #ì—­ì „íŒŒ\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                (100. * batch_idx)/len(train_loader),\n",
    "                loss.item()))\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f'Epoch: {epoch} Average loss: {avg_loss:.6f}')\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c5052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader, device, INPUT_DIM):\n",
    "    model.eval() # í‰ê°€ ëª¨ë“œ ì„¤ì •\n",
    "    test_loss = 0\n",
    "    \n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        data = data.view(data.shape[0], INPUT_DIM)\n",
    "        data = data.view(data.shape[0], INPUT_DIM)\n",
    "            # 1. ğŸš¨ [ìˆ˜ì •] Log-Likelihood ê³„ì‚°ì„ ìœ„í•´ ì—­ë³€í™˜(Inverse)ì„ ìˆ˜í–‰í•´ì•¼ í•¨\n",
    "        z0, logpz_0 = model.inverse(data)\n",
    "            \n",
    "            # 2. Log-Likelihood ê³„ì‚°\n",
    "        logpz = get_logpz(z0.view(z0.size(0), -1))\n",
    "        logpx = logpz + logpz_0\n",
    "        loss = -torch.mean(logpx)\n",
    "\n",
    "            # 3. ì†ì‹¤ ëˆ„ì  (í•œ ì—í¬í¬ ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•´)\n",
    "        test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    # ë°°ì¹˜ ë³„ ì†ì‹¤ì˜ í‰ê·  ê³„ì‚°\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f'====> Test set: Average loss: {avg_loss:.6f}')\n",
    "    \n",
    "    return avg_loss # ìµœì¢… ì†ì‹¤ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c74d7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FFJORD Training Started on cuda ---\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1086.512695\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 893.727905\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 836.170654\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 828.956848\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 813.554077\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 816.059570\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 812.542542\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 805.478333\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 792.387817\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 777.653931\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 759.867065\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 741.365479\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 744.338623\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 725.931335\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 718.538635\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 717.352661\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 710.812500\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 701.134949\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 711.139648\n",
      "Epoch: 1 Average loss: 778.874264\n",
      "====> Test set: Average loss: 694.480752\n",
      "âœ… Loss ê°œì„  í™•ì¸: 694.4808ë¡œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 700.566833\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 690.095276\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 671.377930\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 686.409851\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 657.025879\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 666.256531\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 665.855103\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 646.119629\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 648.134583\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 646.789185\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 622.757446\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 615.441711\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 622.999939\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 609.056152\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 601.166565\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 598.143311\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 559.489380\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 549.035950\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 550.347778\n",
      "Epoch: 2 Average loss: 630.147449\n",
      "====> Test set: Average loss: 543.251890\n",
      "âœ… Loss ê°œì„  í™•ì¸: 543.2519ë¡œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 575.166382\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 539.155151\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 512.672241\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 509.303528\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 477.074493\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 425.910706\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 386.978210\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 412.304688\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 411.333771\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 404.791504\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 387.657593\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 369.071991\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 342.536316\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 345.653015\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 364.753845\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 283.969208\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 299.583740\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 292.457428\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 330.815277\n",
      "Epoch: 3 Average loss: 395.660269\n",
      "====> Test set: Average loss: 268.967942\n",
      "âœ… Loss ê°œì„  í™•ì¸: 268.9679ë¡œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 220.729156\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 230.813446\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 209.569519\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 204.125305\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 204.168030\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 192.709930\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 131.510437\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 146.801147\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 88.015823\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 137.557053\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 108.335869\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 1.981055\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -43.100758\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: -28.816933\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -29.792633\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: -85.446198\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 70.515823\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 52.965977\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -12.501389\n",
      "Epoch: 4 Average loss: 75.584644\n",
      "====> Test set: Average loss: -97.326801\n",
      "âœ… Loss ê°œì„  í™•ì¸: -97.3268ë¡œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: -220.927567\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: -51.179321\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 65.681496\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 3.245508\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 15.363550\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: -77.688377\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -257.238525\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: -136.923401\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -355.542816\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: -83.564194\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -91.598099\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: -327.806641\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -141.253235\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: -196.011963\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -405.589325\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: -234.825897\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -400.012665\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: -376.279175\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -578.052368\n",
      "Epoch: 5 Average loss: -236.016315\n",
      "====> Test set: Average loss: -411.365849\n",
      "âœ… Loss ê°œì„  í™•ì¸: -411.3658ë¡œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: -356.683350\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: -471.186340\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -440.621368\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: -289.653748\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -351.954742\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: -582.997620\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -636.083862\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: -414.734924\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -375.241913\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: -604.367798\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -292.799347\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: -647.175842\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -704.553284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- FFJORD Training Started on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# [í›ˆë ¨] train í•¨ìˆ˜ í˜¸ì¶œ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINPUT_DIM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     train_history.append(train_loss)\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# [í‰ê°€] test í•¨ìˆ˜ í˜¸ì¶œ\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch, model, optimizer, train_loader, device, INPUT_DIM)\u001b[39m\n\u001b[32m     19\u001b[39m loss = -torch.mean(logpx)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#ì—­ì „íŒŒ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     23\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° í™˜ê²½ ì„¤ì •\n",
    "# ----------------------------------------------------------------------\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "INPUT_DIM = 28 * 28  # MNIST ì´ë¯¸ì§€ í‰íƒ„í™” í¬ê¸°\n",
    "TIME_T = 1.0         # ODE ì ë¶„ ì‹œê°„ T\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs('./logs', exist_ok=True) # ë¡œê·¸ í´ë” ìƒì„±\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "# ----------------------------------------------------------------------\n",
    "# (ê³ ê°ë‹˜ì˜ ODEFuncì™€ CNF í´ë˜ìŠ¤ê°€ ì´ì „ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)\n",
    "# ODEFuncëŠ” FFJORDì˜ f(z, t)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "ode_func = ODEFunc(dim=INPUT_DIM).to(device)\n",
    "ode_func = ode_func.float()\n",
    "# CNFëŠ” ODEFuncë¥¼ ë°›ì•„ odeint ì†”ë²„ë¥¼ í†µí•´ íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "model = CNF(ode_func, T=TIME_T).to(device)\n",
    "model = model.float()\n",
    "\n",
    "# Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. ë©”ì¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë£¨í”„\n",
    "# ----------------------------------------------------------------------\n",
    "train_history = []\n",
    "test_history = []\n",
    "best_test_loss = float('inf')\n",
    "print(f\"--- FFJORD Training Started on {device} ---\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    \n",
    "    # [í›ˆë ¨] train í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    train_loss = train(epoch, model, optimizer, train_loader, device, INPUT_DIM)\n",
    "    train_history.append(train_loss)\n",
    "    \n",
    "    # [í‰ê°€] test í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    test_loss = test(epoch, model, test_loader, device, INPUT_DIM)\n",
    "    test_history.append(test_loss)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'test_loss': best_test_loss,\n",
    "        }, './logs/ffjord_best_NORM.pth7') # <-- ì•ˆì •ì ì¸ ëª¨ë¸ íŒŒì¼ ì €ì¥\n",
    "        print(f\"âœ… Loss ê°œì„  í™•ì¸: {best_test_loss:.4f}ë¡œ ëª¨ë¸ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "    # [ëª¨ë¸ ì €ì¥] (ì„ íƒ ì‚¬í•­)\n",
    "    # if test_loss == min(test_history):\n",
    "    #     torch.save(model.state_dict(), './logs/ffjord_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a775cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90ac0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loss -411.3658ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ.\n",
      "\n",
      "âœ… ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì–´ '5ffjord_generated_images_no_logit_fix2_.png'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# (CNF ë° ODEFunc í´ë˜ìŠ¤ëŠ” ì´ì „ì— ì •ì˜ëœ ì…€ì— ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. ì„¤ì •ê°’ ë° ëª¨ë¸ êµ¬ì¡° ì •ì˜\n",
    "# ----------------------------------------------------\n",
    "NUM_IMAGES_TO_GENERATE = 16 \n",
    "INPUT_DIM = 28 * 28 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TIME_T = 1.0 # í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ T ê°’ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•´ êµ¬ì¡°ë¶€í„° ìƒì„±)\n",
    "# âš ï¸ ì£¼ì˜: ODEFuncì™€ CNF í´ë˜ìŠ¤ëŠ” ì´ ì…€ë³´ë‹¤ ìœ„ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    ode_func = ODEFunc(dim=INPUT_DIM).to(device).float()\n",
    "    model = CNF(ode_func, T=TIME_T).to(device).float()\n",
    "except NameError:\n",
    "    print(\"âŒ ì˜¤ë¥˜: ODEFunc ë˜ëŠ” CNF í´ë˜ìŠ¤ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. í•™ìŠµëœ Loss 550 ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (í•„ìˆ˜)\n",
    "# ----------------------------------------------------\n",
    "CHECKPOINT_PATH = './logs/ffjord_best_NORM.pth7' # ğŸŸ¢ ì €ì¥ëœ Loss 550ëŒ€ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_loss = checkpoint.get('test_loss', 563.41) # test_lossê°€ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ 563.41 ì‚¬ìš©\n",
    "    print(f\"âœ… Loss {best_loss:.4f}ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ.\")\n",
    "    \n",
    "    # ëª¨ë¸ ê°ì²´ì— Loss ê°’ì„ ì €ì¥í•˜ì—¬ ì‹œê°í™”ì— ì‚¬ìš©\n",
    "    model.current_loss = best_loss \n",
    "except FileNotFoundError:\n",
    "    print(f\"âš ï¸ ê²½ê³ : ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ '{CHECKPOINT_PATH}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    best_loss = 780.0 # ì´ˆê¸° Loss ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}. ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    best_loss = 780.0\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜ ì •ì˜\n",
    "# ----------------------------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# (CNF ë° ODEFunc í´ë˜ìŠ¤ëŠ” ì´ì „ì— ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜ ì •ì˜ (Logit Transform ë³µì›)\n",
    "# ----------------------------------------------------\n",
    "def generate_and_visualize(model, num_images, input_dim, device):\n",
    "    \n",
    "    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "\n",
    "    # 1. Latent ë²¡í„° ìƒì„± (z0 ~ N(0, I))\n",
    "    z0 = torch.randn(num_images, input_dim).to(device).float() \n",
    "    z0.requires_grad_(True) # Trace Estimator ì‘ë™ì„ ìœ„í•´ í•„ìš”\n",
    "    # 2. logpz_0ë¥¼ ì´ˆê¸°í™” (Trace í•­)\n",
    "    zero_logpz = torch.zeros(z0.shape[0], 1).to(device).float() \n",
    "    \n",
    "    # 3. forward pass ì‹¤í–‰: x_genì€ ì •ê·œí™”ëœ [-1, 1] ë²”ìœ„ì— ìˆìŒ\n",
    "    input_tuple = (z0, zero_logpz)\n",
    "    x_gen, _ = model.inverse(*input_tuple) \n",
    "    \n",
    "    # í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì •ê·œí™” ê°’ (ì „ì²˜ë¦¬ ë‹¨ê³„ì˜ NORM_MEAN, NORM_STDì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤)\n",
    "    NORM_MEAN = 0.5\n",
    "    NORM_STD = 0.5\n",
    "    \n",
    "    # ğŸŸ¢ 1. ì—­ì •ê·œí™” (Denormalization): [-1, 1] -> [0, 1] ë²”ìœ„ë¡œ ë³µì›\n",
    "    x_denorm = (x_gen * NORM_STD) + NORM_MEAN\n",
    "    \n",
    "    # ğŸŸ¢ 2. ìµœì¢… í´ë¦¬í•‘: [0, 1] ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê°’ í´ë¦¬í•‘\n",
    "    # ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  ì´ë¯¸ì§€ í”½ì…€ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ë¡œ ê°•ì œí•©ë‹ˆë‹¤.\n",
    "    x_denorm_clipped = torch.clamp(x_denorm, min=0.0, max=1.0) \n",
    "\n",
    "    # 4. ì´ë¯¸ì§€ í˜•íƒœë¡œ ë³µì› ë° CPU ì´ë™ (í´ë¦¬í•‘ëœ í…ì„œ ì‚¬ìš©)\n",
    "    x_images = x_denorm_clipped.detach().view(num_images, 28, 28).cpu().numpy()\n",
    "    \n",
    "    # 5. ì‹œê°í™” ë° íŒŒì¼ ì €ì¥\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "    \n",
    "    # Loss ê°’ í‘œì‹œ (ë¡œë“œëœ Loss ì‚¬ìš©)\n",
    "    try:\n",
    "        loss_display = f\"Loss: {model.current_loss:.2f}\"\n",
    "    except AttributeError:\n",
    "        # ëª¨ë¸ì— current_lossê°€ ì—†ì„ ê²½ìš°ì˜ ëŒ€ì²´ ê°’ ì‚¬ìš© (ìƒˆë¡œìš´ í•™ìŠµ ì‹œ)\n",
    "        loss_display = \"Loss: N/A (New Training)\"\n",
    "        \n",
    "    plt.suptitle(f\"FFJORD Generated Images ({loss_display})\", fontsize=14)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_images:\n",
    "            # vmin/vmaxë¥¼ 0ê³¼ 1ë¡œ ì„¤ì •í•˜ì—¬ ì˜¬ë°”ë¥¸ í‘ë°± ëª…ì•”ë¹„ë¡œ í‘œì‹œ\n",
    "            ax.imshow(x_images[i], cmap='gray', vmin=0, vmax=1)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # ğŸŸ¢ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥\n",
    "    output_filename = \"5ffjord_generated_images_no_logit_fix2_.png\"\n",
    "    plt.savefig(output_filename, dpi=300) \n",
    "    plt.close(fig)\n",
    "    print(f\"\\nâœ… ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì–´ '{output_filename}'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì‹œ (ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ë§ˆì§€ë§‰ì— ì¶”ê°€)\n",
    "# ----------------------------------------------------\n",
    "generate_and_visualize(model, NUM_IMAGES_TO_GENERATE, INPUT_DIM, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
