{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torchdiffeq import  odeint\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Batch_Size = 32\n",
    "data_dir = './data'\n",
    "os.makedirs(data_dir, exist_ok=True) # ë°ì´í„° í´ë” ìƒì„±\n",
    "\n",
    "INPUT_DIM = 28 * 28 \n",
    "TIME_T = 1.0\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. FFJORD ìµœì í™” ì „ì²˜ë¦¬ ì •ì˜ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)\n",
    "# ----------------------------------------------------------------------\n",
    "# MNIST ë°ì´í„°ì˜ í‰ê· (Mean)ê³¼ í‘œì¤€íŽ¸ì°¨(Std)ëŠ” 0.5ë¥¼ ì‚¬ìš©í•˜ì—¬ [-1, 1] ë²”ìœ„ë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.\n",
    "NORM_MEAN = 0.5\n",
    "NORM_STD = 0.5\n",
    "DEQUANTIZATION_VALUE = 1.0 / 256.0 # 8ë¹„íŠ¸ ì´ë¯¸ì§€ì˜ ìµœì†Œ ê°„ê²©\n",
    "\n",
    "transform_stable = transforms.Compose([\n",
    "    transforms.ToTensor(), # ì´ë¯¸ì§€ë¥¼ [0, 1] í…ì„œë¡œ ë³€í™˜ (Float)\n",
    "    \n",
    "    # ðŸŸ¢ 1. Dequantization (ì—°ì† ë¶„í¬ ê°€ì • ì¶©ì¡±):\n",
    "    # ì´ì‚°ì ì¸ í”½ì…€ ê°’ì— ë¬´ìž‘ìœ„ ë…¸ì´ì¦ˆë¥¼ ë”í•´ ì—°ì†ì ì¸ ë¶„í¬ë¥¼ ë§Œë“­ë‹ˆë‹¤. (í•„ìˆ˜)\n",
    "    lambda x: x + DEQUANTIZATION_VALUE * torch.rand_like(x),\n",
    "    \n",
    "    # ðŸŸ¢ 2. Normalization (ë¶„í¬ ì¤‘ì‹¬ ë§žì¶”ê¸°):\n",
    "    # ë°ì´í„°ë¥¼ [-1, 1] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ ì¡°ì •í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ìž…ë‹ˆë‹¤.\n",
    "    transforms.Normalize((NORM_MEAN,), (NORM_STD,)) \n",
    "])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. ë°ì´í„° ë¡œë” ì •ì˜ (ìˆ˜ì •ëœ transform ì ìš©)\n",
    "# ----------------------------------------------------------------------\n",
    "train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform_stable)\n",
    "test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform_stable)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Batch_Size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca06223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ìž…ë ¥ìœ¼ë¡œ f(x,t)ì™€ y, të¥¼ ë°›ì•„ í—ˆí‚¨ìŠ¨ ì¶”ì •ëŸ‰ì„ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜\n",
    "#ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•˜ì—¬ í‰ê· ì„ ë‚´ì–´ ì •í™•ë„ë¥¼ ë†’ìž„\n",
    "def Huchinson_estimator(forwardzt, y,  num_epsilons):\n",
    "    Batch_Size=y.shape[0]\n",
    "    grad_avg=torch.zeros(y.shape[0],1).to(device).to(y.dtype)\n",
    "    for i in range(num_epsilons):\n",
    "        epsilon=torch.randn_like(y).to(device).to(y.dtype)\n",
    "        grad = torch.autograd.grad(forwardzt, y, grad_outputs=epsilon, create_graph=True)[0]\n",
    "        trace_estimate = torch.sum(grad*epsilon, dim=1, keepdim=True)\n",
    "        grad_avg += trace_estimate\n",
    "    grad_avg /= num_epsilons\n",
    "    return grad_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tê¹Œì§€ ode solve í•´ì£¼ê³  z(t)ì™€ trace ë°˜í™˜. Huchinson_estimator ì‚¬ìš©\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(dim+1, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, dim),\n",
    "        ) \n",
    "        self.dims = dim\n",
    "\n",
    "    def forward1(self, t, x):\n",
    "        t_tensor = torch.ones(x.shape[0],1).to(x.device)*t.to(x.dtype)\n",
    "        xt=torch.cat([x, t_tensor], dim=1)\n",
    "        return self.net(xt)\n",
    "    def forward(self, t, x):\n",
    "        z, logpz_T = x\n",
    "        t=t.to(z.dtype)\n",
    "        if z.dim() > 2:\n",
    "            z_flat = z.view(z.shape[0], -1)\n",
    "        else:\n",
    "            z_flat = z\n",
    "        forwardzt=self.forward1(t,z_flat)\n",
    "        trace = Huchinson_estimator(forwardzt, z, num_epsilons=32)\n",
    "        return forwardzt, -trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eebd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì „ì²´ ì‹ ê²½ë§\n",
    "class CNF(nn.Module):\n",
    "    def __init__(self, ode_func, T=1.0):\n",
    "        super().__init__()\n",
    "        self.ode_func = ode_func\n",
    "        self.time_t = T\n",
    "        self.rtol=1e-4\n",
    "        self.atol=1e-5\n",
    "    #ìˆœë°©í–¥ ì ë¶„ ìƒì„±ê³¼ì •\n",
    "    def forward(self, z0, logpz=None):\n",
    "        if logpz is None:\n",
    "            logpz = torch.zeros(z0.shape[0],1).to(z0.device)\n",
    "        time_tensor=torch.tensor([0.0, self.time_t], dtype=torch.float).to(z0.device)\n",
    "        state_t = odeint(\n",
    "            self.ode_func,\n",
    "            (z0, logpz),\n",
    "            time_tensor,\n",
    "            method='rk4',\n",
    "            rtol=self.rtol,\n",
    "            atol=self.atol,\n",
    "            options={}\n",
    "        )\n",
    "        zT=state_t[0][-1]\n",
    "        logpz_T=state_t[1][-1]\n",
    "\n",
    "        return zT, logpz_T\n",
    "    #ì—­ë°©í–¥ ì ë¶„ í•™ìŠµê³¼ì •\n",
    "    def inverse(self, zT, logpz_T=None):\n",
    "        if logpz_T is None:\n",
    "            logpz_T = torch.zeros(zT.shape[0],1).to(zT.device)\n",
    "        time_tensor=torch.tensor([self.time_t, 0.0],dtype=torch.float).to(zT.device)\n",
    "        state_t = odeint( \n",
    "            #ìœ„ì— êµ¬í˜„í•œ ODEFunc ì‚¬ìš©           \n",
    "            self.ode_func,\n",
    "            # ODEì˜ ì´ˆê¸° ìƒíƒœ (zT, logpz_T(traceì— í•´ë‹¹))\n",
    "            (zT, logpz_T),\n",
    "            #ì‹œê°„ ì—­ë°©í–¥ìœ¼ë¡œ ì ë¶„\n",
    "            time_tensor,\n",
    "            method='rk4',\n",
    "            rtol=self.rtol,\n",
    "            atol=self.atol,\n",
    "            options={}\n",
    "        )\n",
    "        #ê°ê° forwardztì™€ traceê°€ ì ë¶„ëœ ê²°ê³¼ ë°˜í™˜\n",
    "        z0 = state_t[0][-1]      # ìµœì¢… ìƒíƒœ z0 (t=0, ë…¸ì´ì¦ˆ)\n",
    "        logpz_0 = state_t[1][-1] # ìµœì¢… ë¡œê·¸ í™•ë¥  ë³´ì¡° í•­ (log p(x) ê³„ì‚°ì— ì‚¬ìš©)\n",
    "        \n",
    "        return z0, logpz_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8a8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í‘œì¤€ ì •ê·œë¶„í¬ì˜ ë¡œê·¸ í™•ë¥  ë°€ë„ í•¨ìˆ˜ ê³„ì‚°\n",
    "def get_logpz(z):\n",
    "    logpz = -0.5 * torch.sum(z**2, dim=1, keepdim=True) - 0.5 * z.shape[1] * torch.tensor(np.log(2 * np.pi)).to(device)\n",
    "    return logpz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adbecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (epoch, model, optimizer, train_loader, device, INPUT_DIM):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data=data.to(device).to(torch.float32)\n",
    "        data = data + (torch.rand_like(data) * (1/256.0))\n",
    "        #z1, z2, z3..... ì— ê°€ëŠ¥í•œ ëª¨ë“  ìˆ«ìžì— í™•ë¥ ì´ ìžˆì–´ì•¼ í•œë‹¤.\n",
    "        data=data.view(data.shape[0], INPUT_DIM)\n",
    "        data.requires_grad_(True)\n",
    "\n",
    "        #ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "        #ëª¨ë¸ì˜ ì—­ë°©í–¥ ì ë¶„ ìˆ˜í–‰(ë°ì´íƒ€ë¶„í¬->í‘œì¤€ì •ê·œë¶„í¬) ì´ z0ì´ ë‚˜ì˜¬í™•ë¥ ì„ ìµœëŒ€í™” í•´ì•¼í•œë‹¤.\n",
    "        z0, logpz_0 = model.inverse(data)\n",
    "        #z0ì— ëŒ€í•œ í‘œì¤€ ì •ê·œë¶„í¬ì˜ ë¡œê·¸ í™•ë¥  ë°€ë„ í•¨ìˆ˜ ê³„ì‚°\n",
    "        logpz = get_logpz(z0)\n",
    "        #log p(x)= logp(z0) + log|det(dz0/dx)|(ì¦‰ -traceì˜ ì ë¶„ê°’)\n",
    "        logpx = logpz + logpz_0\n",
    "        #ë°°ì¹˜ì•ˆ ì „ì²´ì˜ lossí•¨ìˆ˜ í‰ê·  ê³„ì‚°\n",
    "        loss = -torch.mean(logpx)\n",
    "        #ì—­ì „íŒŒ\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                (100. * batch_idx)/len(train_loader),\n",
    "                loss.item()))\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f'Epoch: {epoch} Average loss: {avg_loss:.6f}')\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, model, test_loader, device, INPUT_DIM):\n",
    "    model.eval() # í‰ê°€ ëª¨ë“œ ì„¤ì •\n",
    "    test_loss = 0\n",
    "    \n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device).to(torch.float32)\n",
    "        data = data + (torch.rand_like(data) * (1/256.0))\n",
    "        data = data.view(data.shape[0], INPUT_DIM)\n",
    "        data.requires_grad_(True)\n",
    "\n",
    "            # 1. ðŸš¨ [ìˆ˜ì •] Log-Likelihood ê³„ì‚°ì„ ìœ„í•´ ì—­ë³€í™˜(Inverse)ì„ ìˆ˜í–‰í•´ì•¼ í•¨\n",
    "        z0, logpz_0 = model.inverse(data)\n",
    "            \n",
    "            # 2. Log-Likelihood ê³„ì‚°\n",
    "        logpz = get_logpz(z0)\n",
    "        logpx = logpz + logpz_0\n",
    "        loss = -torch.mean(logpx)\n",
    "\n",
    "            # 3. ì†ì‹¤ ëˆ„ì  (í•œ ì—í¬í¬ ëª¨ë“  ë°°ì¹˜ì— ëŒ€í•´)\n",
    "        test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "    # ë°°ì¹˜ ë³„ ì†ì‹¤ì˜ í‰ê·  ê³„ì‚°\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f'====> Test set: Average loss: {avg_loss:.6f}')\n",
    "    \n",
    "    return avg_loss # ìµœì¢… ì†ì‹¤ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74d7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FFJORD Training Started on cuda ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\autograd\\graph.py:865: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:330.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 758.454041\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 760.969910\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 756.181519\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 762.735352\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 757.014832\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 758.794800\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 762.802246\n",
      "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 753.772766\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 746.707031\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 741.580566\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 737.659241\n",
      "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 737.665894\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 726.299927\n",
      "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 733.415833\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 728.417236\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 722.335022\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 724.651306\n",
      "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 718.016724\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 711.387817\n",
      "Train Epoch: 1 [15200/60000 (25%)]\tLoss: 712.170227\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 707.817383\n",
      "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 701.994263\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 700.707642\n",
      "Train Epoch: 1 [18400/60000 (31%)]\tLoss: 702.675842\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 695.108398\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 697.107544\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 691.220215\n",
      "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 693.567627\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 684.550171\n",
      "Train Epoch: 1 [23200/60000 (39%)]\tLoss: 676.483276\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 686.098389\n",
      "Train Epoch: 1 [24800/60000 (41%)]\tLoss: 678.356689\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 677.090454\n",
      "Train Epoch: 1 [26400/60000 (44%)]\tLoss: 676.701050\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 675.713257\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 670.406311\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 672.355469\n",
      "Train Epoch: 1 [29600/60000 (49%)]\tLoss: 663.633057\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 665.497070\n",
      "Train Epoch: 1 [31200/60000 (52%)]\tLoss: 663.075073\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 669.711426\n",
      "Train Epoch: 1 [32800/60000 (55%)]\tLoss: 668.028442\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 656.495850\n",
      "Train Epoch: 1 [34400/60000 (57%)]\tLoss: 656.782349\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 652.014038\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 657.630432\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 652.023926\n",
      "Train Epoch: 1 [37600/60000 (63%)]\tLoss: 645.290649\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 656.054688\n",
      "Train Epoch: 1 [39200/60000 (65%)]\tLoss: 647.599731\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 647.730957\n",
      "Train Epoch: 1 [40800/60000 (68%)]\tLoss: 651.923706\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 648.933594\n",
      "Train Epoch: 1 [42400/60000 (71%)]\tLoss: 641.783875\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 644.164917\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 639.341492\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 642.698792\n",
      "Train Epoch: 1 [45600/60000 (76%)]\tLoss: 640.543152\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 634.731934\n",
      "Train Epoch: 1 [47200/60000 (79%)]\tLoss: 633.592957\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 641.627075\n",
      "Train Epoch: 1 [48800/60000 (81%)]\tLoss: 638.879883\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 644.329346\n",
      "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 647.205139\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 629.062744\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 627.010498\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 628.541016\n",
      "Train Epoch: 1 [53600/60000 (89%)]\tLoss: 624.584595\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 623.632507\n",
      "Train Epoch: 1 [55200/60000 (92%)]\tLoss: 625.931580\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 622.596924\n",
      "Train Epoch: 1 [56800/60000 (95%)]\tLoss: 624.273865\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 624.530273\n",
      "Train Epoch: 1 [58400/60000 (97%)]\tLoss: 617.997864\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 616.335449\n",
      "Epoch: 1 Average loss: 678.064923\n",
      "====> Test set: Average loss: 618.461725\n",
      "âœ… Loss ê°œì„  í™•ì¸: 618.4617ë¡œ ëª¨ë¸ ì €ìž¥ ì™„ë£Œ.\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 618.110352\n",
      "Train Epoch: 2 [800/60000 (1%)]\tLoss: 614.704407\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 623.049622\n",
      "Train Epoch: 2 [2400/60000 (4%)]\tLoss: 617.159180\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 610.785156\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 616.585815\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 613.640869\n",
      "Train Epoch: 2 [5600/60000 (9%)]\tLoss: 604.515503\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 609.079468\n",
      "Train Epoch: 2 [7200/60000 (12%)]\tLoss: 609.274292\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 611.295532\n",
      "Train Epoch: 2 [8800/60000 (15%)]\tLoss: 600.578979\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 604.615356\n",
      "Train Epoch: 2 [10400/60000 (17%)]\tLoss: 608.577576\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 606.121216\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 597.609009\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 609.640869\n",
      "Train Epoch: 2 [13600/60000 (23%)]\tLoss: 596.453796\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 592.028809\n",
      "Train Epoch: 2 [15200/60000 (25%)]\tLoss: 600.218994\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 588.643188\n",
      "Train Epoch: 2 [16800/60000 (28%)]\tLoss: 588.795166\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 587.764404\n",
      "Train Epoch: 2 [18400/60000 (31%)]\tLoss: 593.230591\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 590.430725\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 588.924561\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 580.752136\n",
      "Train Epoch: 2 [21600/60000 (36%)]\tLoss: 593.081543\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 583.918213\n",
      "Train Epoch: 2 [23200/60000 (39%)]\tLoss: 584.498474\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 584.793274\n",
      "Train Epoch: 2 [24800/60000 (41%)]\tLoss: 582.786255\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 581.851440\n",
      "Train Epoch: 2 [26400/60000 (44%)]\tLoss: 578.119446\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 577.802612\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 582.719971\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 583.949341\n",
      "Train Epoch: 2 [29600/60000 (49%)]\tLoss: 588.862427\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 566.458008\n",
      "Train Epoch: 2 [31200/60000 (52%)]\tLoss: 581.550415\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 569.844482\n",
      "Train Epoch: 2 [32800/60000 (55%)]\tLoss: 589.840210\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 581.961548\n",
      "Train Epoch: 2 [34400/60000 (57%)]\tLoss: 569.755249\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 569.429443\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 577.501587\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 557.693359\n",
      "Train Epoch: 2 [37600/60000 (63%)]\tLoss: 568.173706\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 562.776306\n",
      "Train Epoch: 2 [39200/60000 (65%)]\tLoss: 567.795898\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 571.411255\n",
      "Train Epoch: 2 [40800/60000 (68%)]\tLoss: 563.571777\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 571.398376\n",
      "Train Epoch: 2 [42400/60000 (71%)]\tLoss: 572.019409\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 547.391235\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 560.441895\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 561.738770\n",
      "Train Epoch: 2 [45600/60000 (76%)]\tLoss: 558.987915\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 555.095215\n",
      "Train Epoch: 2 [47200/60000 (79%)]\tLoss: 574.020142\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 552.572693\n",
      "Train Epoch: 2 [48800/60000 (81%)]\tLoss: 557.922852\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 566.401001\n",
      "Train Epoch: 2 [50400/60000 (84%)]\tLoss: 555.162354\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 552.421753\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 547.265930\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 539.265015\n",
      "Train Epoch: 2 [53600/60000 (89%)]\tLoss: 542.389526\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 549.968994\n",
      "Train Epoch: 2 [55200/60000 (92%)]\tLoss: 535.262024\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 559.767212\n",
      "Train Epoch: 2 [56800/60000 (95%)]\tLoss: 560.422729\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 546.042725\n",
      "Train Epoch: 2 [58400/60000 (97%)]\tLoss: 548.352173\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 556.122314\n",
      "Epoch: 2 Average loss: 579.897860\n",
      "====> Test set: Average loss: 546.507180\n",
      "âœ… Loss ê°œì„  í™•ì¸: 546.5072ë¡œ ëª¨ë¸ ì €ìž¥ ì™„ë£Œ.\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 538.482544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- FFJORD Training Started on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m     54\u001b[39m \n\u001b[32m     55\u001b[39m     \u001b[38;5;66;03m# [í›ˆë ¨] train í•¨ìˆ˜ í˜¸ì¶œ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINPUT_DIM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     train_history.append(train_loss)\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# [í‰ê°€] test í•¨ìˆ˜ í˜¸ì¶œ\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch, model, optimizer, train_loader, device, INPUT_DIM)\u001b[39m\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#ëª¨ë¸ì˜ ì—­ë°©í–¥ ì ë¶„ ìˆ˜í–‰(ë°ì´íƒ€ë¶„í¬->í‘œì¤€ì •ê·œë¶„í¬) ì´ z0ì´ ë‚˜ì˜¬í™•ë¥ ì„ ìµœëŒ€í™” í•´ì•¼í•œë‹¤.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m z0, logpz_0 = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#z0ì— ëŒ€í•œ í‘œì¤€ ì •ê·œë¶„í¬ì˜ ë¡œê·¸ í™•ë¥  ë°€ë„ í•¨ìˆ˜ ê³„ì‚°\u001b[39;00m\n\u001b[32m     16\u001b[39m logpz = get_logpz(z0)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mCNF.inverse\u001b[39m\u001b[34m(self, zT, logpz_T)\u001b[39m\n\u001b[32m     30\u001b[39m     logpz_T = torch.zeros(zT.shape[\u001b[32m0\u001b[39m],\u001b[32m1\u001b[39m).to(zT.device)\n\u001b[32m     31\u001b[39m time_tensor=torch.tensor([\u001b[38;5;28mself\u001b[39m.time_t, \u001b[32m0.0\u001b[39m],dtype=torch.float).to(zT.device)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m state_t = \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#ìœ„ì— êµ¬í˜„í•œ ODEFunc ì‚¬ìš©           \u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mode_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ODEì˜ ì´ˆê¸° ìƒíƒœ (zT, logpz_T(traceì— í•´ë‹¹))\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mzT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogpz_T\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#ì‹œê°„ ì—­ë°©í–¥ìœ¼ë¡œ ì ë¶„\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrk4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m#ê°ê° forwardztì™€ traceê°€ ì ë¶„ëœ ê²°ê³¼ ë°˜í™˜\u001b[39;00m\n\u001b[32m     45\u001b[39m z0 = state_t[\u001b[32m0\u001b[39m][-\u001b[32m1\u001b[39m]      \u001b[38;5;66;03m# ìµœì¢… ìƒíƒœ z0 (t=0, ë…¸ì´ì¦ˆ)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\odeint.py:80\u001b[39m, in \u001b[36modeint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[39m\n\u001b[32m     77\u001b[39m solver = SOLVERS[method](func=func, y0=y0, rtol=rtol, atol=atol, **options)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     solution = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     event_t, solution = solver.integrate_until_event(t[\u001b[32m0\u001b[39m], event_fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\solvers.py:114\u001b[39m, in \u001b[36mFixedGridODESolver.integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m    112\u001b[39m dt = t1 - t0\n\u001b[32m    113\u001b[39m \u001b[38;5;28mself\u001b[39m.func.callback_step(t0, y0, dt)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m dy, f0 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m y1 = y0 + dy\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m j < \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;129;01mand\u001b[39;00m t1 >= t[j]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\fixed_grid.py:29\u001b[39m, in \u001b[36mRK4._step_func\u001b[39m\u001b[34m(self, func, t0, dt, t1, y0)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_step_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, t0, dt, t1, y0):\n\u001b[32m     28\u001b[39m     f0 = func(t0, y0, perturb=Perturb.NEXT \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.perturb \u001b[38;5;28;01melse\u001b[39;00m Perturb.NONE)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrk4_alt_step_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m)\u001b[49m, f0\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py:115\u001b[39m, in \u001b[36mrk4_alt_step_func\u001b[39m\u001b[34m(func, t0, dt, t1, y0, f0, perturb)\u001b[39m\n\u001b[32m    113\u001b[39m k2 = func(t0 + dt * _one_third, y0 + dt * k1 * _one_third)\n\u001b[32m    114\u001b[39m k3 = func(t0 + dt * _two_thirds, y0 + dt * (k2 - k1 * _one_third))\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m k4 = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk1\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mk3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPerturb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPREV\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPerturb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (k1 + \u001b[32m3\u001b[39m * (k2 + k3) + k4) * dt * \u001b[32m0.125\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\misc.py:197\u001b[39m, in \u001b[36m_PerturbFunc.forward\u001b[39m\u001b[34m(self, t, y, perturb)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\misc.py:165\u001b[39m, in \u001b[36m_ReverseFunc.forward\u001b[39m\u001b[34m(self, t, y)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mul * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torchdiffeq\\_impl\\misc.py:144\u001b[39m, in \u001b[36m_TupleFunc.forward\u001b[39m\u001b[34m(self, t, y)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_flat_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([f_.reshape(-\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mODEFunc.forward\u001b[39m\u001b[34m(self, t, x)\u001b[39m\n\u001b[32m     26\u001b[39m     z_flat = z\n\u001b[32m     27\u001b[39m forwardzt=\u001b[38;5;28mself\u001b[39m.forward1(t,z_flat)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m trace = \u001b[43mHuchinson_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwardzt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epsilons\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m forwardzt, -trace\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mHuchinson_estimator\u001b[39m\u001b[34m(forwardzt, y, num_epsilons)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epsilons):\n\u001b[32m      7\u001b[39m     epsilon=torch.randn_like(y).to(device).to(y.dtype)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     grad = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwardzt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m     trace_estimate = torch.sum(grad*epsilon, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m     grad_avg += trace_estimate\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\autograd\\__init__.py:515\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    511\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    512\u001b[39m         grad_outputs_\n\u001b[32m    513\u001b[39m     )\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    526\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    527\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    528\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\harryjo\\Desktop\\hwiwon ffjord\\venv313\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° í™˜ê²½ ì„¤ì •\n",
    "# ----------------------------------------------------------------------\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "INPUT_DIM = 28 * 28  # MNIST ì´ë¯¸ì§€ í‰íƒ„í™” í¬ê¸°\n",
    "TIME_T = 1.0         # ODE ì ë¶„ ì‹œê°„ T\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs('./logs', exist_ok=True) # ë¡œê·¸ í´ë” ìƒì„±\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. ë°ì´í„° ë¡œë” ì •ì˜\n",
    "# ----------------------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜ [0, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "# ----------------------------------------------------------------------\n",
    "# (ê³ ê°ë‹˜ì˜ ODEFuncì™€ CNF í´ëž˜ìŠ¤ê°€ ì´ì „ì— ì •ì˜ë˜ì–´ ìžˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)\n",
    "# ODEFuncëŠ” FFJORDì˜ f(z, t)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "ode_func = ODEFunc(dim=INPUT_DIM).to(device)\n",
    "ode_func = ode_func.float()\n",
    "# CNFëŠ” ODEFuncë¥¼ ë°›ì•„ odeint ì†”ë²„ë¥¼ í†µí•´ íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "model = CNF(ode_func, T=TIME_T).to(device)\n",
    "model = model.float()\n",
    "\n",
    "# Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. ë©”ì¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë£¨í”„\n",
    "# ----------------------------------------------------------------------\n",
    "train_history = []\n",
    "test_history = []\n",
    "best_test_loss = float('inf')\n",
    "print(f\"--- FFJORD Training Started on {device} ---\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    \n",
    "    # [í›ˆë ¨] train í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    train_loss = train(epoch, model, optimizer, train_loader, device, INPUT_DIM)\n",
    "    train_history.append(train_loss)\n",
    "    \n",
    "    # [í‰ê°€] test í•¨ìˆ˜ í˜¸ì¶œ\n",
    "    test_loss = test(epoch, model, test_loader, device, INPUT_DIM)\n",
    "    test_history.append(test_loss)\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'test_loss': best_test_loss,\n",
    "        }, './logs/ffjord_best_NORM.pth6') # <-- ì•ˆì •ì ì¸ ëª¨ë¸ íŒŒì¼ ì €ìž¥\n",
    "        print(f\"âœ… Loss ê°œì„  í™•ì¸: {best_test_loss:.4f}ë¡œ ëª¨ë¸ ì €ìž¥ ì™„ë£Œ.\")\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "    # [ëª¨ë¸ ì €ìž¥] (ì„ íƒ ì‚¬í•­)\n",
    "    # if test_loss == min(test_history):\n",
    "    #     torch.save(model.state_dict(), './logs/ffjord_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a775cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ac0ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loss 546.5072ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ.\n",
      "\n",
      "âœ… ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì–´ 'ffjord_generated_images_final_NORM2tanhnumepupupup.png'ìœ¼ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# (CNF ë° ODEFunc í´ëž˜ìŠ¤ëŠ” ì´ì „ì— ì •ì˜ëœ ì…€ì— ì¡´ìž¬í•´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. ì„¤ì •ê°’ ë° ëª¨ë¸ êµ¬ì¡° ì •ì˜\n",
    "# ----------------------------------------------------\n",
    "NUM_IMAGES_TO_GENERATE = 16 \n",
    "INPUT_DIM = 28 * 28 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TIME_T = 1.0 # í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ T ê°’ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° ì •ì˜ (ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•´ êµ¬ì¡°ë¶€í„° ìƒì„±)\n",
    "# âš ï¸ ì£¼ì˜: ODEFuncì™€ CNF í´ëž˜ìŠ¤ëŠ” ì´ ì…€ë³´ë‹¤ ìœ„ì— ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    ode_func = ODEFunc(dim=INPUT_DIM).to(device).float()\n",
    "    model = CNF(ode_func, T=TIME_T).to(device).float()\n",
    "except NameError:\n",
    "    print(\"âŒ ì˜¤ë¥˜: ODEFunc ë˜ëŠ” CNF í´ëž˜ìŠ¤ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. í•™ìŠµëœ Loss 550 ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ (í•„ìˆ˜)\n",
    "# ----------------------------------------------------\n",
    "CHECKPOINT_PATH = './logs/ffjord_best_NORM.pth6' # ðŸŸ¢ ì €ìž¥ëœ Loss 550ëŒ€ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_loss = checkpoint.get('test_loss', 563.41) # test_lossê°€ ìžˆìœ¼ë©´ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ 563.41 ì‚¬ìš©\n",
    "    print(f\"âœ… Loss {best_loss:.4f}ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ.\")\n",
    "    \n",
    "    # ëª¨ë¸ ê°ì²´ì— Loss ê°’ì„ ì €ìž¥í•˜ì—¬ ì‹œê°í™”ì— ì‚¬ìš©\n",
    "    model.current_loss = best_loss \n",
    "except FileNotFoundError:\n",
    "    print(f\"âš ï¸ ê²½ê³ : ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ '{CHECKPOINT_PATH}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    best_loss = 780.0 # ì´ˆê¸° Loss ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}. ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    best_loss = 780.0\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜ ì •ì˜\n",
    "# ----------------------------------------------------\n",
    "def generate_and_visualize(model, num_images, input_dim, device):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # 1. Latent ë²¡í„° ìƒì„± (z0 ~ N(0, I))\n",
    "    z0 = torch.randn(num_images, input_dim).to(device).float() \n",
    "    \n",
    "    # ðŸŸ¢ FINAL FIX #1: z0ì— requires_grad_(True)ë¥¼ ë‹¤ì‹œ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "    z0.requires_grad_(True) \n",
    "\n",
    "    # 2. logpz_0ë¥¼ ì´ˆê¸°í™” (Trace í•­)\n",
    "    zero_logpz = torch.zeros(z0.shape[0], 1).to(device).float() \n",
    "    \n",
    "    # 3. forward pass ì‹¤í–‰\n",
    "    input_tuple = (z0, zero_logpz)\n",
    "    x_gen, _ = model.forward(*input_tuple) \n",
    "\n",
    "    # 4. ì´ë¯¸ì§€ í˜•íƒœë¡œ ë³µì› ë° CPU ì´ë™\n",
    "    # ðŸŸ¢ FINAL FIX #3: detach()ë¥¼ ì‚¬ìš©í•˜ì—¬ numpy() í˜¸ì¶œ ì „ ë¯¸ë¶„ ê¸°ë¡ ë¶„ë¦¬\n",
    "    x_images = x_gen.detach().view(num_images, 28, 28).cpu().numpy() \n",
    "\n",
    "    # 5. ì‹œê°í™” ë° íŒŒì¼ ì €ìž¥\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "    \n",
    "    # Loss ê°’ í‘œì‹œ (ë¡œë“œëœ Loss ì‚¬ìš©)\n",
    "    try:\n",
    "        loss_display = f\"Loss: {model.current_loss:.2f}\"\n",
    "    except AttributeError:\n",
    "        # ë¡œë“œ ì‹¤íŒ¨ ì‹œ fallback\n",
    "        loss_display = f\"Loss: {best_loss:.2f} (Loaded/Initial)\" \n",
    "        \n",
    "    plt.suptitle(f\"FFJORD Generated Images ({loss_display})\", fontsize=14)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_images:\n",
    "            # í”½ì…€ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ë¡œ í´ë¦¬í•‘í•˜ì—¬ ëª…ì•”ë¹„ ì¡°ì •\n",
    "            ax.imshow(x_images[i], cmap='gray', vmin=0, vmax=1)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    # ðŸŸ¢ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ìž¥\n",
    "    output_filename = \"ffjord_generated_images_final_NORM2tanhnumepupupup.png\"\n",
    "    plt.savefig(output_filename, dpi=300) \n",
    "    plt.close(fig)\n",
    "    print(f\"\\nâœ… ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì–´ '{output_filename}'ìœ¼ë¡œ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. í•¨ìˆ˜ í˜¸ì¶œ\n",
    "# ----------------------------------------------------\n",
    "generate_and_visualize(model, NUM_IMAGES_TO_GENERATE, INPUT_DIM, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
